import json
import re
from collections import defaultdict
from difflib import SequenceMatcher

# === –ü–ê–†–ê–ú–ï–¢–†–´ ===
PASTA_MIN_LENGTH = 10  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–∞—Å—Ç—ã
SIMILARITY_THRESHOLD = 0.8  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏

import os
from pathlib import Path
import json
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å—Ä–µ–¥—ã
load_dotenv()

# –ü–æ–ª—É—á–∞–µ–º –∫–æ—Ä–Ω–µ–≤–æ–π –ø—É—Ç—å –ø—Ä–æ–µ–∫—Ç–∞ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
PROJECT_ROOT = os.getenv("PROJECT_ROOT")

if not PROJECT_ROOT:
    raise ValueError("PROJECT_ROOT –Ω–µ –∑–∞–¥–∞–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è.")


# –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —á–∞—Ç–∞ –ø–æ stream_id
def load_chat_data(stream_id):
    """–§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —á–∞—Ç–∞ –ø–æ stream_id (–∑–∞–≥–ª—É—à–∫–∞, –Ω—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ)."""
    # –°—Ç—Ä–æ–∏–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
    file_path = Path(PROJECT_ROOT) / "stream_data" / f"{stream_id}.json"

    if not file_path.exists():
        raise FileNotFoundError(f"–§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")

    with open(file_path, "r", encoding="utf-8") as file:
        return json.load(file)["chat"]


# –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è (–∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é).
PROJECT_ROOT = os.getenv("PROJECT_ROOT", Path(__file__).resolve().parent)

def load_chat_data(stream_id):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —á–∞—Ç–∞ –ø–æ stream_id."""
    file_path = Path(PROJECT_ROOT) / "stream_data" / f"{stream_id}.json"

    if not file_path.exists():
        raise FileNotFoundError(f"–§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")

    with open(file_path, "r", encoding="utf-8") as file:
        return json.load(file)["chat"]


def normalize_text(text):
    """–ü—Ä–∏–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –∫ —Ñ–æ—Ä–º–µ –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–≥–∏—Å—Ç—Ä"""
    return re.sub(r"[^\w\d]+", "", text)


def extract_pastas(chat_data):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø–∞—Å—Ç—ã –∏–∑ —á–∞—Ç–∞"""
    pasta_counter = defaultdict(list)

    for msg in chat_data:
        text = msg["message"]["body"]
        msg_id = msg["_id"]

        if len(text) < PASTA_MIN_LENGTH:
            continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã

        pasta_counter[text].append(msg_id)

    return {text: ids for text, ids in pasta_counter.items() if len(ids) > 1}


def group_similar_pastas(pasta_data):
    """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø–∞—Å—Ç—ã –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏"""
    grouped_pastas = []
    seen = set()

    for base_pasta, message_ids in sorted(pasta_data.items(), key=lambda x: -len(x[1])):  # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é
        if base_pasta in seen:
            continue

        base_normalized = normalize_text(base_pasta)
        group = {
            "base_pasta": base_pasta,
            "count": len(message_ids),
            "messages": message_ids,
            "variants": []
        }

        for other_pasta, other_ids in pasta_data.items():
            if other_pasta in seen or other_pasta == base_pasta:
                continue

            other_normalized = normalize_text(other_pasta)
            similarity = SequenceMatcher(None, base_normalized, other_normalized).ratio()

            if similarity >= SIMILARITY_THRESHOLD:
                group["variants"].append({
                    "text": other_pasta,
                    "count": len(other_ids),
                    "messages": other_ids
                })
                seen.add(other_pasta)

        grouped_pastas.append(group)
        seen.add(base_pasta)

    return grouped_pastas


def get_pastas_for_stream(stream_id):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –∑–∞–≥—Ä—É–∂–∞–µ—Ç —á–∞—Ç, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Å—Ç—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Ö –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏."""
    chat_data = load_chat_data(stream_id)
    pastas = extract_pastas(chat_data)
    grouped_pastas = group_similar_pastas(pastas)
    return grouped_pastas


# === –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø ===
if __name__ == "__main__":
    stream_id = "2406296141"  # ID –Ω—É–∂–Ω–æ–π —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏
    result = get_pastas_for_stream(stream_id)

    print("üîç –¢–û–ü-10 –ü–ê–°–¢:")
    for i, pasta in enumerate(result[:10], 1):
        print(f"{i}. {pasta['base_pasta']} ({pasta['count']} –ø–æ–≤—Ç–æ—Ä–æ–≤)")
        if pasta["variants"]:
            print("   üîπ –í–∞—Ä–∏–∞–Ω—Ç—ã:")
            for variant in pasta["variants"]:
                print(f"      - {variant['text']} ({variant['count']} –ø–æ–≤—Ç–æ—Ä–æ–≤)")