import json
import re
from collections import defaultdict
from difflib import SequenceMatcher
import os
from pathlib import Path
from dotenv import load_dotenv

# === –ü–ê–†–ê–ú–ï–¢–†–´ ===
PASTA_MIN_LENGTH = 10  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–∞—Å—Ç—ã
SIMILARITY_THRESHOLD = 0.8  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å—Ä–µ–¥—ã
# –ó–∞–≥—Ä—É–∑–∏–º .env.local.local –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
if os.environ.get('FLASK_ENV') == 'development':
    load_dotenv('.env.local')
else:
    load_dotenv('.env.docker')

# –ü–æ–ª—É—á–∞–µ–º –∫–æ—Ä–Ω–µ–≤–æ–π –ø—É—Ç—å –ø—Ä–æ–µ–∫—Ç–∞ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
PROJECT_ROOT = os.getenv("PROJECT_ROOT")

if not PROJECT_ROOT:
    raise ValueError("PROJECT_ROOT –Ω–µ –∑–∞–¥–∞–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è.")

# –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —á–∞—Ç–∞ –ø–æ stream_id
def load_chat_data(stream_id):
    """–§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —á–∞—Ç–∞ –ø–æ stream_id"""
    # –°—Ç—Ä–æ–∏–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
    file_path = Path(PROJECT_ROOT) / "stream_data" / f"{stream_id}.json"

    if not file_path.exists():
        raise FileNotFoundError(f"–§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")

    with open(file_path, "r", encoding="utf-8") as file:
        return json.load(file)["chat"]

# –§—É–Ω–∫—Ü–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
def normalize_text(text):
    """–ü—Ä–∏–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –∫ —Ñ–æ—Ä–º–µ –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–≥–∏—Å—Ç—Ä"""
    return re.sub(r"[^\w\d]+", "", text)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Å—Ç –∏–∑ —á–∞—Ç–∞
def extract_pastas(chat_data):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø–∞—Å—Ç—ã –∏–∑ —á–∞—Ç–∞"""
    pasta_counter = defaultdict(list)

    for msg in chat_data:
        text = msg["message"]["body"]
        msg_id = msg["_id"]

        if len(text) < PASTA_MIN_LENGTH:
            continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã

        pasta_counter[text].append(msg_id)

    return {text: ids for text, ids in pasta_counter.items() if len(ids) > 1}

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–∞—Å—Ç –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏
def group_similar_pastas(pasta_data):
    """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø–∞—Å—Ç—ã –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏"""
    grouped_pastas = []
    seen = set()

    for base_pasta, message_ids in sorted(pasta_data.items(), key=lambda x: -len(x[1])):  # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é
        if base_pasta in seen:
            continue

        base_normalized = normalize_text(base_pasta)
        group = {
            "base_pasta": base_pasta,
            "count": len(message_ids),
            "variants": []
        }

        for other_pasta, other_ids in pasta_data.items():
            if other_pasta in seen or other_pasta == base_pasta:
                continue

            other_normalized = normalize_text(other_pasta)
            similarity = SequenceMatcher(None, base_normalized, other_normalized).ratio()

            if similarity >= SIMILARITY_THRESHOLD:
                group["variants"].append({
                    "text": other_pasta,
                    "count": len(other_ids)
                })
                seen.add(other_pasta)

        grouped_pastas.append(group)
        seen.add(base_pasta)

    return grouped_pastas

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–∞—Å—Ç
def get_pastas_for_stream(stream_id, top_n=None):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –∑–∞–≥—Ä—É–∂–∞–µ—Ç —á–∞—Ç, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Å—Ç—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Ö –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏."""
    chat_data = load_chat_data(stream_id)
    pastas = extract_pastas(chat_data)
    grouped_pastas = group_similar_pastas(pastas)
    return grouped_pastas[:top_n]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–≤–æ–¥–∏–º—ã—Ö –ø–∞—Å—Ç

# === –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø ===
if __name__ == "__main__":
    stream_id = "2406296141"  # ID –Ω—É–∂–Ω–æ–π —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏
    top_n = 10  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Å—Ç –¥–ª—è –≤—ã–≤–æ–¥–∞

    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Å—Ç
    result = get_pastas_for_stream(stream_id, top_n=top_n)

    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –Ω—É–∂–Ω–æ–π —Ñ–æ—Ä–º–µ
    print(f"üîç –¢–û–ü-{top_n} –ü–ê–°–¢:")
    for i, pasta in enumerate(result, 1):
        print(f"{i}. –ë–∞–∑–æ–≤–∞—è –ø–∞—Å—Ç–∞: {pasta['base_pasta']} ‚Äî –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {pasta['count']}")
        if pasta["variants"]:
            print("   üîπ –ü–æ–¥–ø–∞—Å—Ç—ã:")
            for variant in pasta["variants"]:
                print(f"      - {variant['text']} ‚Äî –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {variant['count']}")